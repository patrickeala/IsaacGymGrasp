import pickle
import torch

import matplotlib.pyplot as plt

v1 = pickle.load(open('dd1.pkl','rb'))
v2 = pickle.load(open('dd2.pkl','rb'))

v1 = torch.stack(v1, axis=0).cpu()
v2 = torch.stack(v2, axis=0).cpu()

print(v1.shape, v2.shape)




# print(v1[10:20, :, 0])
# print(v2[10:20, :, 0])
print(torch.equal(v1, v2))

# for i in range(142):
#     if not torch.equal(v1[:,i,2], v2[:,i,2]):
#         print(i)
#         # print(v1[:,i,2])
#         # print(v2[:,i,2])


# fig, ax = plt.subplots()
# # for i in range(142):
# print(v1[10:20,0,2].shape)
# ax.plot(v1[10:20,0,2])
# ax.plot(v2[10:20,0,2])
# plt.show()
# a1 = torch.tensor([    0.0000000102,     0.0000000678,     0.0000005018,     0.0087104570,
#             0.0000010382,     0.0000005862,     0.0000002583,     0.0000004797,
#             0.0000004589,     0.0000000070,     0.0000019897,     0.0000001669,
#             0.0000002360,     0.0000002296,     0.0000004447,     0.0000004638,
#             0.0000006821,     0.0148545122,     0.0000004695,     0.0000011423,
#             0.0000003579,     0.0000017418,     0.0000001027,     0.0000001504,
#             0.0165623911,     0.0000003042,     0.0153269544,     0.0000005031,
#             0.0000005348,     0.0000005917,     0.0000005264,     0.0000023839,
#             0.0000010817,     0.0142477471,     0.0000003224,     0.0000006025,
#             0.0000004563,     0.0000006276,     0.0000006797,     0.0000006120,
#             0.0000007586,     0.0000005977,     0.0000012126,     0.0000008790,
#             0.0000007501,     0.0000008366,     0.0000008831,     0.0145687573,
#             0.0000006657,     0.0000010740,     0.0000011676,     0.0000012009,
#             0.0000011697,     0.0134036122,     0.0000011886,     0.0000009349,
#             0.0172443911,     0.0000011888,     0.0000008013,     0.0154078426,
#             0.0000002378,     0.0000011960,     0.0152892489,     0.0000011528,
#             0.0000013132,     0.0000013946,     0.0105159860,     0.0000009461,
#             0.0000009801,     0.0000009878,     0.0000007813,     0.0000010572,
#             0.0125936568,     0.0000012183,     0.0000007869,     0.0000014401,
#             0.0000018522,     0.0000005792,     0.0000009822,     0.0000008505,
#             0.0000010739,     0.0000008657,     0.0000012549,     0.0000012254,
#             0.0000012994,     0.0132363029,     0.0000012126,     0.0000014426,
#             0.0148088690,     0.0000012818,     0.0000024217,     0.0000011212,
#             0.0000028728,     0.0117506348,     0.0000021574,     0.0000028681,
#             0.0000027739,     0.0000022519,     0.0000019693,     0.0000018133,
#             0.0000016380,     0.0000004208,     0.0133572351,     0.0000013125,
#             0.0000027994,     0.0000022716,     0.0000018001,     0.0000007868,
#             0.0124561768,     0.0000022269,     0.0000023626,     0.0000013741,
#             0.0000013837,     0.0000025140,     0.0000010585,     0.0000024571,
#             0.0000018774,     0.0000018746,     0.0000021124,     0.0000018495,
#             0.0000017760,     0.0000022615,     0.0000016931,     0.0000019740,
#             0.0000013182,     0.0000024325,     0.0000009297,     0.0129237529,
#             0.0000015485,     0.0000009742,     0.0000029433,     0.0156093631,
#             0.0000028848,     0.0000006454,     0.0000016789,     0.0000016634,
#             0.0160249695,     0.0000018106,     0.0112822121,     0.0000015490,
#             0.0000021033,     0.0156937800], device='cuda:0')

# a2 = torch.tensor([    0.0000000102,     0.0000000678,     0.0000005018,     0.0087104570,
#             0.0000010382,     0.0000005862,     0.0000002583,     0.0000004797,
#             0.0000004589,     0.0000000070,     0.0000019897,     0.0000001669,
#             0.0000002360,     0.0000002296,     0.0000004447,     0.0000004638,
#             0.0000006821,     0.0148545122,     0.0000004695,     0.0000011423,
#             0.0000003579,     0.0000017418,     0.0000001027,     0.0000001504,
#             0.0165623911,     0.0000003042,     0.0153269544,     0.0000005031,
#             0.0000005348,     0.0000005917,     0.0000005264,     0.0000023839,
#             0.0000010817,     0.0142477471,     0.0000003224,     0.0000006025,
#             0.0000004563,     0.0000006276,     0.0000006797,     0.0000006120,
#             0.0000007586,     0.0000005977,     0.0000012126,     0.0000008790,
#             0.0000007501,     0.0000008366,     0.0000008831,     0.0145687573,
#             0.0000006657,     0.0000010740,     0.0000011676,     0.0000012009,
#             0.0000011697,     0.0134036122,     0.0000011886,     0.0000009349,
#             0.0172443911,     0.0000011888,     0.0000008013,     0.0154078426,
#             0.0000002378,     0.0000011960,     0.0152892489,     0.0000011528,
#             0.0000013132,     0.0000013946,     0.0105159860,     0.0000009461,
#             0.0000009801,     0.0000009878,     0.0000007813,     0.0000010572,
#             0.0125936568,     0.0000012183,     0.0000007869,     0.0000014401,
#             0.0000018522,     0.0000005792,     0.0000009822,     0.0000008505,
#             0.0000010739,     0.0000008657,     0.0000012549,     0.0000012254,
#             0.0000012994,     0.0132363029,     0.0000012126,     0.0000014426,
#             0.0148088690,     0.0000012818,     0.0000024217,     0.0000011212,
#             0.0000028728,     0.0117506348,     0.0000021574,     0.0000028681,
#             0.0000027739,     0.0000022519,     0.0000019693,     0.0000018133,
#             0.0000016380,     0.0000004208,     0.0133572351,     0.0000013125,
#             0.0000027994,     0.0000022716,     0.0000018001,     0.0000007868,
#             0.0124561768,     0.0000022269,     0.0000023626,     0.0000013741,
#             0.0000013837,     0.0000025140,     0.0000010585,     0.0000024571,
#             0.0000018774,     0.0000018746,     0.0000021124,     0.0000018495,
#             0.0000017760,     0.0000022615,     0.0000016931,     0.0000019740,
#             0.0000013182,     0.0000024325,     0.0000009297,     0.0129237529,
#             0.0000015485,     0.0000009742,     0.0000029433,     0.0156093631,
#             0.0000028848,     0.0000006454,     0.0000016789,     0.0000016634,
#             0.0160249695,     0.0000018106,     0.0112822121,     0.0000015490,
#             0.0000021033,     0.0156937800], device='cuda:0')

# labels1 = torch.where(a1 > 0.0005, 1.0, 0.0) 
# labels2 = torch.where(a2 > 0.0005, 1.0, 0.0) 

# print(torch.sum(labels1))
# print(torch.sum(labels2))
# # for i, (_a1, _a2) in enumerate(zip(a1, a2)):
# #     print(_a1.item(), _a2.item(), (_a1==_a2).item())
# #     print(labels1[0], labels2[0])
# #     print('----------------')

